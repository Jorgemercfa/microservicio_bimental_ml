import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import json
import re
import logging
import requests
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import tokenizer_from_json

# -----------------------------
# CONFIGURACI√ìN
# -----------------------------
# CORRECCI√ìN: Cambiado a OPENAI_API_KEY
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_API_URL = "https://api.openai.com/v1/chat/completions"

# Configuraci√≥n de logging MEJORADA
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(), 
        logging.FileHandler("dass21_api.log", encoding="utf-8")
    ]
)
logger = logging.getLogger("dass21")

# √çndices DASS-21 (1-based)
DEP_INDICES = [3, 5, 10, 13, 16, 17, 21]
ANS_INDICES = [2, 4, 7, 9, 15, 19, 20]
EST_INDICES = [1, 6, 8, 11, 12, 14, 18]

# -----------------------------
# NORMALIZADOR Y MAPEO DE ETIQUETAS
# -----------------------------
def normalize_text_basic(text: str):
    if not isinstance(text, str):
        text = str(text)
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Mapeo directo como fallback
LABEL_MAP_SPANISH = {
    "no": 0, "nunca": 0, "casi nunca": 0,
    "raras veces": 1, "ocasionalmente": 1, "algunas veces": 1, "s√≠, poco": 1, "si, poco": 1,
    "bastante": 2, "frecuentemente": 2, "usualmente": 2, "casi siempre": 2, "s√≠, bastante": 2, "si, bastante": 2,
    "siempre": 3, "s√≠, mucho": 3, "si, mucho": 3
}

# -----------------------------
# MODELOS DE REQUEST
# -----------------------------
class TextListRequest(BaseModel):
    respuestas: List[str]

class ChatRequest(BaseModel):
    message: str

# -----------------------------
# PREDICTOR DASS-21 OPTIMIZADO (MANTENIDO PARA FUTURAS MEJORAS)
# -----------------------------
class DASS21Predictor:
    def __init__(self, model_path: str, tokenizer_path: str, max_len: int = None):
        # Este c√≥digo se mantiene por si en el futuro se soluciona la compatibilidad
        # pero actualmente NO se usar√°
        pass

    def _predict_question(self, text: str) -> int:
        # Usar siempre el mapeo directo (m√°s confiable)
        key = normalize_text_basic(text)
        prediction = LABEL_MAP_SPANISH.get(key, 1)  # Default a 1 si no se encuentra
        logger.info(f"Predicci√≥n para: '{text}' -> {prediction}")
        return prediction

    def predict_scores_from_text_list(self, respuestas: List[str]) -> Dict[str, int]:
        if not isinstance(respuestas, list) or len(respuestas) != 21:
            raise ValueError("Se requieren exactamente 21 respuestas para el cuestionario DASS-21.")
        
        per_question_vals = [self._predict_question(r) for r in respuestas]

        dep_total = sum(per_question_vals[i-1] for i in DEP_INDICES)
        ans_total = sum(per_question_vals[i-1] for i in ANS_INDICES)
        est_total = sum(per_question_vals[i-1] for i in EST_INDICES)

        return {
            "depresion": dep_total,
            "ansiedad": ans_total,
            "estres": est_total,
            "respuestas": per_question_vals,
            "nota": "Usando mapeo directo (sistema confiable)"
        }

# -----------------------------
# PREDICTOR ALTERNATIVO MEJORADO (PRINCIPAL)
# -----------------------------
class AlternativeDASS21Predictor:
    """Predictor principal que usa solo el mapeo de etiquetas - M√ÅS CONFIABLE"""
    def __init__(self):
        logger.info("‚úÖ Predictor principal inicializado (mapeo directo de etiquetas)")
        
    def _predict_question(self, text: str) -> int:
        key = normalize_text_basic(text)
        prediction = LABEL_MAP_SPANISH.get(key, 1)  # Default a 1 si no se encuentra
        logger.info(f"Predicci√≥n para: '{text}' -> {prediction}")
        return prediction
    
    def predict_scores_from_text_list(self, respuestas: List[str]) -> Dict[str, int]:
        if not isinstance(respuestas, list) or len(respuestas) != 21:
            raise ValueError("Se requieren exactamente 21 respuestas.")
        
        per_question_vals = [self._predict_question(r) for r in respuestas]
        
        dep_total = sum(per_question_vals[i-1] for i in DEP_INDICES)
        ans_total = sum(per_question_vals[i-1] for i in ANS_INDICES)
        est_total = sum(per_question_vals[i-1] for i in EST_INDICES)

        return {
            "depresion": dep_total,
            "ansiedad": ans_total,
            "estres": est_total,
            "respuestas": per_question_vals,
            "nota": "Sistema confiable - Mapeo directo de etiquetas"
        }

# -----------------------------
# FASTAPI APP
# -----------------------------
app = FastAPI(title="DASS-21 API", version="1.0")

# CORS configurado
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# SOLUCI√ìN: Usar siempre el predictor alternativo (m√°s confiable)
predictor = None

@app.on_event("startup")
def load_model():
    global predictor
    try:
        # ‚úÖ FORZAR el uso del predictor alternativo (CONFIABLE)
        predictor = AlternativeDASS21Predictor()
        logger.info("‚úÖ Predictor principal inicializado exitosamente")
        
        # Probar una predicci√≥n simple
        test_result = predictor._predict_question("no")
        logger.info(f"‚úÖ Prueba de predicci√≥n exitosa: 'no' -> {test_result}")
        
    except Exception as e:
        logger.exception("‚ùå Error inesperado en inicializaci√≥n")
        # Fallback extremo
        predictor = AlternativeDASS21Predictor()
        logger.info("‚úÖ Predictor de respaldo inicializado")

# -----------------------------
# ENDPOINTS (SIN CAMBIOS)
# -----------------------------
@app.post("/predict/")
def predict_scores(request: TextListRequest):
    if not request.respuestas or not isinstance(request.respuestas, list):
        raise HTTPException(status_code=400, detail="Se requiere una lista de respuestas (no vac√≠a)")
    if len(request.respuestas) != 21:
        raise HTTPException(status_code=422, detail="Debe enviar exactamente 21 respuestas para el DASS-21.")
    try:
        result = predictor.predict_scores_from_text_list(request.respuestas)
        return result
    except Exception as e:
        logger.exception("Error al predecir los puntajes")
        raise HTTPException(status_code=500, detail=f"Error al predecir los puntajes: {str(e)}")

# -----------------------------
# FUNCI√ìN PARA OPENAI - MEJORADA
# -----------------------------
def consultar_openai(mensaje: str) -> str:
    # Verificar si la API Key est√° configurada
    if not OPENAI_API_KEY:
        logger.error("‚ùå OPENAI_API_KEY no configurada en variables de entorno")
        return "Lo siento, el servicio de chat no est√° configurado correctamente. Por favor, contacta al administrador del sistema."
    
    try:
        payload = {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "system", "content": "Eres un asistente emp√°tico especializado en salud mental. Responde de forma breve, clara y respetuosa."},
                {"role": "user", "content": mensaje}
            ],
            "max_tokens": 200,
            "temperature": 0.7,
        }
        
        logger.info(f"Enviando consulta a OpenAI: {mensaje[:50]}...")
        
        response = requests.post(
            OPENAI_API_URL,
            headers={
                "Content-Type": "application/json", 
                "Authorization": f"Bearer {OPENAI_API_KEY}"
            },
            json=payload,
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            respuesta = data["choices"][0]["message"]["content"].strip()
            logger.info("‚úÖ Respuesta de OpenAI recibida correctamente")
            return respuesta
        else:
            logger.error(f"‚ùå Error OpenAI API: {response.status_code} - {response.text}")
            return "Lo siento, no puedo responder en este momento. Por favor intenta m√°s tarde."
            
    except requests.exceptions.Timeout:
        logger.error("‚è∞ Timeout al consultar OpenAI")
        return "El servicio est√° tardando demasiado en responder. Por favor intenta nuevamente."
    except Exception as e:
        logger.error(f"‚ùå Error consultando OpenAI: {e}")
        return "Lo siento, hay un problema temporal con el servicio. Por favor intenta m√°s tarde."

@app.post("/chat/")
def chat_with_user(request: ChatRequest):
    try:
        user_message = request.message.strip()
        if not user_message:
            raise HTTPException(status_code=400, detail="El mensaje no puede estar vac√≠o")
        
        logger.info(f"üì® Mensaje recibido en /chat: {user_message}")
        respuesta = consultar_openai(user_message)
        
        return {"respuesta": respuesta}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("‚ùå Error en el endpoint de chat")
        raise HTTPException(status_code=500, detail=f"Error interno del servidor: {str(e)}")

# Endpoint de salud MEJORADO
@app.get("/health")
def health_check():
    status = "ok" if predictor is not None else "error"
    message = "API DASS-21 funcionando correctamente" if predictor else "Predictor no inicializado"
    
    # Verificar tambi√©n la API Key de OpenAI
    openai_status = "configured" if OPENAI_API_KEY else "not configured"
    
    return {
        "status": status, 
        "message": message,
        "version": "1.0",
        "predictor_type": predictor.__class__.__name__ if predictor else "None",
        "openai_api_key": openai_status,
        "endpoints_available": ["/predict", "/chat", "/model-info"]
    }

# Endpoint de informaci√≥n del modelo
@app.get("/model-info")
def model_info():
    if predictor is None:
        raise HTTPException(status_code=503, detail="Predictor no inicializado")
    
    model_info = {
        "model_loaded": True,
        "predictor_type": predictor.__class__.__name__,
        "openai_configured": bool(OPENAI_API_KEY),
        "system": "Mapeo directo de etiquetas - Sistema confiable"
    }
    return model_info

# Endpoint ra√≠z
@app.get("/")
def read_root():
    return {
        "message": "API DASS-21 Predictor", 
        "version": "1.0",
        "endpoints": [
            {"path": "/health", "method": "GET", "description": "Estado del servicio"},
            {"path": "/predict", "method": "POST", "description": "Evaluaci√≥n DASS-21"},
            {"path": "/chat", "method": "POST", "description": "Chat de apoyo emocional"},
            {"path": "/model-info", "method": "GET", "description": "Informaci√≥n del modelo"}
        ],
        "status": "active"
    }

# === SOLUCI√ìN: CONFIGURACI√ìN ESPEC√çFICA PARA RAILWAY ===
# AGREGAR ESTO AL FINAL DEL ARCHIVO

# CONFIGURACI√ìN ESPEC√çFICA PARA RAILWAY
import os
# Forzar el puerto que Railway espera
if "RAILWAY_STATIC_URL" in os.environ:
    os.environ["PORT"] = "8080"

# === main ===
if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8080))  # CAMBIADO de 8000 a 8080 para Railway
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")